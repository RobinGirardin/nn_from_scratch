<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.5.54">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">


<title>Classification Neural Network without TensorFlow or Keras</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { display: inline-block; text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script><script src="script_files/libs/clipboard/clipboard.min.js"></script>
<script src="script_files/libs/quarto-html/quarto.js"></script>
<script src="script_files/libs/quarto-html/popper.min.js"></script>
<script src="script_files/libs/quarto-html/tippy.umd.min.js"></script>
<script src="script_files/libs/quarto-html/anchor.min.js"></script>
<link href="script_files/libs/quarto-html/tippy.css" rel="stylesheet">
<link href="script_files/libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="script_files/libs/bootstrap/bootstrap.min.js"></script>
<link href="script_files/libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="script_files/libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js" integrity="sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==" crossorigin="anonymous"></script>

<script type="application/javascript">define('jquery', [],function() {return window.jQuery;})</script>

  <script src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

</head>

<body class="fullcontent">

<div id="quarto-content" class="page-columns page-rows-contents page-layout-article">

<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<h1 class="title">Classification Neural Network without TensorFlow or Keras</h1>
</div>



<div class="quarto-title-meta">

    
  
    
  </div>
  


</header>


<p>Author: Robin Girardin</p>
<p>Date : 31.12.2024</p>
<section id="table-of-content" class="level2">
<h2 class="anchored" data-anchor-id="table-of-content">Table of Content</h2>
<ol type="1">
<li><p><a href="#presentation-of-the-project">Presentation of the project</a></p></li>
<li><p><a href="#forward-propagation">Forward Propagation</a></p>
<p>2.1. <a href="#linear-transformation">Linear Transformation</a></p>
<p>2.2. <a href="#activation-function">Activation Function</a></p>
<p>2.3. <a href="#weight-matrix">Weight Matrix</a></p>
<p>2.4. <a href="#input-matrix">Input Matrix</a></p>
<p>2.5. <a href="#bias-vector">Bias Vector</a></p>
<p>2.6. <a href="#computation">Computation</a></p>
<p>2.6.1. <a href="#linear-transformation">Linear Transformation</a></p>
<p>2.6.2. <a href="#activation-function">Activation Function</a></p>
<p>2.7. <a href="#relu">ReLU</a></p>
<p>2.8. <a href="#softmax">Softmax</a></p>
<p>2.9. <a href="#forward-pass">Forward pass</a></p></li>
<li><p><a href="#backward-propagation">Backward Propagation</a></p>
<p>3.1.<a href="#weight-updating">Weight updating</a></p>
<p>3.2. <a href="#bias-updating">Bias updating</a></p></li>
<li><p><a href="#prediction">Prediction</a></p>
<p>4.1. <a href="#training-performance">Training Performance</a></p>
<p>4.2. <a href="#testing-performance">Testing Performance</a></p></li>
</ol>
</section>
<section id="presentation-of-the-project" class="level2">
<h2 class="anchored" data-anchor-id="presentation-of-the-project">Presentation of the project</h2>
<p>As of late, I’ve been exploring Machine Learning algorithms. It’s time for me to learn more about Deep Learning and Neural Networks. I’ve been learning the math behind those concept and here is my attempt to build a simple Neural Network without the help of any library, except for numpy and pandas. The purpose of this project is to built a solid fundation for future deep learning projects. How can one become an expert at something without building it from scratch? I’ll attempt to explain my thought process throughout the project. The code will be naturally supported by mathematic formulas, so that the reader can judge my understanding of the topic. Beware that I’m no mathematician. Therefore, please excuse me if my notation isn’t conform to the norms. I still hope it will be understandable.</p>
<p>For this, I’ll attempt to code a neural network for a supervised multiclass classification task. The goal is to make the model able to classify numbers from 1 to 9 from the famous <a href="#https://www.kaggle.com/datasets/oddrationale/mnist-in-csv?select=mnist_train.csv">MNIST dataset</a>.</p>
<p>Since each image of the MNIST dataset is composed of 784 pixels, the input layer of our new model will have 784 nodes. Additionnaly, we expecting a classification between 10 numbers (0 included), consequently, the output layer of our new model will have 10 nodes. Finally, we need to define how much nodes and layers our model’s hidden layers will have. Since I want to have a small model at first, I’ll go for a single hidden layer with 10 nodes.</p>
<blockquote class="blockquote">
<p>After finishing the model and training it, there was some overflow issue. I strandardized the data <code>X_train_stand</code> and <code>X_test_stand</code> to solve this issue.</p>
</blockquote>
<div id="e2dbc771" class="cell" data-execution_count="1">
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a>train <span class="op">=</span> np.array(pd.read_csv(<span class="st">"../data/raw/mnist_train.csv"</span>))</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a>test <span class="op">=</span> np.array(pd.read_csv(<span class="st">"../data/raw/mnist_test.csv"</span>))</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a>X_train <span class="op">=</span> np.delete(train, <span class="dv">0</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a>Y_train <span class="op">=</span> train[:, <span class="dv">0</span>]</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a>X_train_stand <span class="op">=</span> X_train <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>)</span>
<span id="cb1-12"><a href="#cb1-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-13"><a href="#cb1-13" aria-hidden="true" tabindex="-1"></a>X_test <span class="op">=</span> np.delete(test, <span class="dv">0</span>, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb1-14"><a href="#cb1-14" aria-hidden="true" tabindex="-1"></a>Y_test <span class="op">=</span> test[:, <span class="dv">0</span>]</span>
<span id="cb1-15"><a href="#cb1-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb1-16"><a href="#cb1-16" aria-hidden="true" tabindex="-1"></a>X_test_stand <span class="op">=</span> X_test <span class="op">*</span> (<span class="dv">1</span> <span class="op">/</span> <span class="dv">255</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="forward-propagation">Forward Propagation</h2>
<p>Forward propagation is the computation of the linear transformation multiplied by the activation function, which is commonly called the hypothesis function. It is composed of two processes : the linear transformation <span class="math inline">\(Z\)</span> and the activation function <span class="math inline">\(\sigma\left(Z\right)\)</span>.</p>
<section id="linear-transformation" class="level3">
<h3 class="anchored" data-anchor-id="linear-transformation">Linear Transformation</h3>
<p>As you can see, the result of the <span class="math inline">\(Z\)</span> is passed down to the activation function. The purpose of the linear transformation is to adjust the input recieved by the previous layer using two important parameter, called weights <span class="math inline">\(W\)</span> and biases <span class="math inline">\(B\)</span>. Each feature (in our case a single pixel) <span class="math inline">\(x_{m, n}\)</span> of each example (the whole image composed of 784 pixels : <span class="math inline">\(x\)</span> (<span class="math inline">\(\begin{bmatrix} x_{1, 1} &amp; x_{1, 2} &amp; \dots &amp; \ x_{1, 784} \end{bmatrix}\)</span>) fed to the model will be adjusted by its own corresponding weight <span class="math inline">\(w_{i, j}\)</span>. This allow a neuron to downplay or increase the importance of specific features <span class="math inline">\(x_{m, n}\)</span> of the example so that its output is a specific combination of the example’s features. The linear transformation function can module the importance of each feature only linearly.</p>
<p>This output acts as a new feature, which will be fed to further layers to create new, more complex, features following the same process. <span class="math inline">\(Z\)</span> acts as a decision boundary separating part of the original data, thus creating a new feature. <span class="math display">\[
Z(X) = WX + B
\]</span> Where: * <span class="math inline">\(W\)</span>: Weight matrix for the layer. * <span class="math inline">\(X\)</span>: Input matrix or to the layer. * <span class="math inline">\(b\)</span>: Bias vector for the layer.</p>
</section>
<section id="activation-function" class="level3">
<h3 class="anchored" data-anchor-id="activation-function">Activation Function</h3>
<p>Due to the linear nature of the pre-activation function <span class="math inline">\(Z\)</span>, multiple issues arise. Firstly, the modulation of feature importance is limited and cannot explain more complex interactions in the data. For example, let’s say a particular feature created by a neuron have a increasingly lower impact on the final output the higher its values are. In order to represent this, you would need to make sure that values outputed by the neuron are capped to a limit, right? This would limit the impact of of said value, aka feature, on the final model.</p>
<p>Secondly, since <span class="math inline">\(Z = W * X + B\)</span> acts as a decision boundary which separates new features, it being linear is inherently limiting. What if, in order to separate a group of points, a “S” shapes curve is more appropriate?</p>
<p>Thirdly, each neuron outputs a value, which is fed to other neurons. This means that <span class="math inline">\(Z_11 = W_1 \cdot X + B_1\)</span> is fed to <span class="math inline">\(Z_2 = W_2 \cdot Z_1 + B_2 = W_2 * \left(W_1 \cdot X + B_1\right) + B_2\)</span>. In the end, even with a lot of hidden layers and neurons, the model can be resumed to a linear function.</p>
<p>All those issues are solved by applying a second step after the linear transformation called activation function <span class="math inline">\(\sigma\left(Z\right)\)</span>. This will map <span class="math inline">\(Z\)</span> value to another non-linear space so that complex neuron relationships and features can be created. There is a lot of different activation function, serving more specific purposes. However, they all add non-linearity to the model and create complexity. <span class="math display">\[
\sigma\left(Z\right) = \sigma\left(WX + B\right)
\]</span></p>
</section>
<section id="weight-matrix" class="level3">
<h3 class="anchored" data-anchor-id="weight-matrix">Weight Matrix</h3>
<p>A weight is a coefficient of importance attributed to the relationship between two neurons. Each neuron recieved information from all the neurons of the previous layer, this means that first neuron of this hidden layer has <span class="math inline">\(i\)</span> different relationships, based on the number of neuron in the input layer. Since each relationship symbolizes the information that the previous neuron transmits to its successor, weight modulation is akin to input importance management. This allow the model to specilize each neuron to a particular feature by maximizing the importance a certain input, while minimizing others. This importance adjustement will be refined through a process called <em>backward propagation</em>. It’ll make sure that each neurons coordinates each other well together and bring out the most usefull information. This is the learning process of the model.</p>
<p><span class="math inline">\(W\)</span> is a matrix of shape <span class="math inline">\(\left(i, j\right)\)</span>, <span class="math inline">\(i\)</span> being the number of inputs that the neurons will recieve from its previous layer and <span class="math inline">\(j\)</span> being the number of neurons in the layer. In our case, the weight matrix of the hidden layer <span class="math inline">\(W_1\)</span> has a shape of <span class="math inline">\(\left(784, 10\right)\)</span>, meaning 784 rows and 10 columns, while the weight matrix <span class="math inline">\(W_2\)</span> has a shape of <span class="math inline">\(\left(10, 10\right)\)</span> Concretely, those a matrices containing the information of all weights of each neuron of a particular layer, either the hidden layer for <span class="math inline">\(W_1\)</span> or the output layer for <span class="math inline">\(W_2\)</span>. <span class="math display">\[
W_1 =
\begin{bmatrix}
w_{1, 1} &amp; w_{1, 2} &amp; \dots &amp; w_{1, 10} \\
w_{2, 1} &amp; w_{2, 2} &amp; \dots &amp; w_{2, 10} \\
w_{3, 1} &amp; w_{3, 2} &amp; \dots &amp; w_{2, 10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
w_{784, 1} &amp; w_{784, 2} &amp; \dots &amp; w_{784, 10} \\
\end{bmatrix}
\]</span> <span class="math display">\[
W_2 =
\begin{bmatrix}
w_{1, 1} &amp; w_{1, 2} &amp; \dots &amp; w_{1, 10} \\
w_{2, 1} &amp; w_{2, 2} &amp; \dots &amp; w_{2, 10} \\
w_{3, 1} &amp; w_{3, 2} &amp; \dots &amp; w_{3, 10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
w_{10, 1} &amp; w_{10, 2} &amp; \dots &amp; w_{10, 10} \\
\end{bmatrix}
\]</span></p>
</section>
<section id="input-matrix" class="level3">
<h3 class="anchored" data-anchor-id="input-matrix">Input Matrix</h3>
<p>The input matrix is the data that will be fed to the model. This data has the form of a matrix. Each row represent generally an observation, while the columns are different “features” of the data ; they are the variable that the model will the interpret. A feature can be a particular category, a score, continuous number such as prices and so on. An observation is a single example described by its different features that the model will learn from. In our case, each row is a single image, while each column is the pixel value of each 784 pixels composing the image. The full training dataset has 60’000 images. <span class="math display">\[
X =
\begin{bmatrix}
x_{1, 1} &amp; x_{1, 2} &amp; \dots &amp; x_{1, 784} \\
x_{2, 1} &amp; x_{2, 2} &amp; \dots &amp; x_{2, 784} \\
x_{3, 1} &amp; x_{3, 2} &amp; \dots &amp; x_{2, 784} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{60000, 1} &amp; x_{60000, 2} &amp; \dots &amp; x_{60000, 784} \\
\end{bmatrix}
\]</span> The previous explanation is true for the first hidden layer, since its input comes from the input layer and each neuron of the input layer is a single feature of the dataset. However, the input matrix of subsequent layers are slightly different. This is because they recieve their input from the computation done by all neurons of the previous layer. In this case, the input matrix’s row keep the same meaning : one row for one input given to the model (an image in our case). However, each column are now each neuron’s output, instead of being each feature of the dataset. More concretely, this matrix <span class="math inline">\(A\)</span> contains the computed results for all given images by each neurons.</p>
<p>This is one the reason why a neural network is powerfull : each layer further derive the results of the previous layer, creating more complex and refined features along the way. <span class="math display">\[
A =
\begin{bmatrix}
a_{1, 1} &amp; a_{1, 2} &amp; \dots &amp; a_{1, 10} \\
a_{2, 1} &amp; a_{2, 2} &amp; \dots &amp; a_{2, 10} \\
a_{3, 1} &amp; a_{3, 2} &amp; \dots &amp; a_{2, 10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
a_{60000, 1} &amp; a_{60000, 2} &amp; \dots &amp; a_{60000, 10} \\
\end{bmatrix}
\]</span></p>
</section>
<section id="bias-vector" class="level3">
<h3 class="anchored" data-anchor-id="bias-vector">Bias vector</h3>
<p>Each neuron has its own bias. Contrary to the weight, which module the importance of the input recieved by the neuron, the bias influence final feature (combination of all input, modulated by their respective weight) created by the neuron itself. If the weight is akin to the proportion of carrots, lettuce, peas in a salad, the bias is the salad’s spiciness determining its flavor as a whole.</p>
<p><span class="math inline">\(B\)</span> is the vector containing the bias for all neurons of a layer, in our case <span class="math inline">\(B_1\)</span> for the hidden layer and <span class="math inline">\(B_2\)</span> for the output layer (input layer doesn’t count since its only purpose is to pass down informations from the dataset, but doesn’t compute anything) <span class="math display">\[
B_1 =
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
\vdots \\
b_{10}
\end{bmatrix}
\]</span> <span class="math display">\[
B_2 =
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
\vdots \\
b_{10}
\end{bmatrix}
\]</span></p>
</section>
<section id="computation" class="level3">
<h3 class="anchored" data-anchor-id="computation">Computation</h3>
<section id="linear-transformation-1" class="level4">
<h4 class="anchored" data-anchor-id="linear-transformation-1">Linear Transformation</h4>
<p>As you can see, the pre-activation function multiply the dot product of the first input vector and each neuron’s weight vector. On a scale of a whole matrix, this is a matrix multiplication. This is done over all the inputs or example (in our cases images), giving out a matrix of shape <span class="math inline">\(\left(60000, 10\right)\)</span> Conceptually, for each neuron, (represented by a column in the matrix), each feature <span class="math inline">\(x_(m, n)\)</span> of a particular example is scaled with its corresponding weight then added to the bias of the neuron. <span class="math display">\[
Z_1(X) = X \cdot W + B
\]</span> <span class="math display">\[
=
\begin{bmatrix}
x_{1, 1} &amp; x_{1, 2} &amp; \dots &amp; x_{1, 784} \\
x_{2, 1} &amp; x_{2, 2} &amp; \dots &amp; x_{2, 784} \\
x_{3, 1} &amp; x_{3, 2} &amp; \dots &amp; x_{2, 784} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
x_{60000, 1} &amp; x_{60000, 2} &amp; \dots &amp; x_{60000, 784} \\
\end{bmatrix}
\cdot
\begin{bmatrix}
w_{1, 1} &amp; w_{1, 2} &amp; \dots &amp; w_{1, 10} \\
w_{2, 1} &amp; w_{2, 2} &amp; \dots &amp; w_{2, 10} \\
w_{3, 1} &amp; w_{3, 2} &amp; \dots &amp; w_{2, 10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
w_{784, 1} &amp; w_{784, 2} &amp; \dots &amp; w_{784, 10} \\
\end{bmatrix}
+
\begin{bmatrix}
b_1 \\
b_2 \\
b_3 \\
\vdots \\
b_{10}
\end{bmatrix}
\]</span> <span class="math display">\[
=
\begin{bmatrix}
\vec{x_{1, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{1, n}}.\vec{w_{i, 2}} + b_2 &amp; \dots &amp; \vec{x_{1, n}}.\vec{w_{i, 10}} + b_{10} \\
\vec{x_{2, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{2, n}}.\vec{w_{i, 2}} + b_2 &amp; \dots &amp; \vec{x_{2, n}}.\vec{w_{i, 10}} + b_{10} \\
\vec{x_{3, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{3, n}}.\vec{w_{i, 2}} + b_2 &amp; \dots &amp; \vec{x_{3, n}}.\vec{w_{i, 10}} + b_{10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\vec{x_{60000, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{60000, n}}.\vec{w_{i, 2}} + b_2 &amp; \dots &amp; \vec{x_{60000, n}}.\vec{w_{i, 10}} + b_{10} \\
\end{bmatrix}
\]</span></p>
</section>
<section id="activation-function-1" class="level4">
<h4 class="anchored" data-anchor-id="activation-function-1">Activation function</h4>
<p><span class="math display">\[
\sigma\left(
\begin{bmatrix}
\vec{x_{1, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{1, n}}.\vec{w_{i, 2}} +  b_2 &amp; \dots &amp; \vec{x_{1, n}}.\vec{w_{i, 10}} +  b_{10} \\
\vec{x_{2, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{2, n}}.\vec{w_{i, 2}} +  b_2 &amp; \dots &amp; \vec{x_{2, n}}.\vec{w_{i, 10}} +  b_{10} \\
\vec{x_{3, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{3, n}}.\vec{w_{i, 2}} +  b_2 &amp; \dots &amp; \vec{x_{3, n}}.\vec{w_{i, 10}} +  b_{10} \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\vec{x_{60000, n}}.\vec{w_{i, 1}} + b_1 &amp; \vec{x_{60000, n}}.\vec{w_{i, 2}} +  b_2 &amp; \dots &amp; \vec{x_{60000, n}}.\vec{w_{i, 10}} +  b_{10} \\
\end{bmatrix}
\right)
\]</span> <span class="math display">\[
=
\begin{bmatrix}
\sigma\left(\vec{x_{1, n}}.\vec{w_{i, 1}} + b_1\right) &amp; \sigma\left(\vec{x_{1, n}}.\vec{w_{i, 2}} + b_2\right) &amp; \dots &amp; \sigma\left(\vec{x_{1, n}}.\vec{w_{i, 10}} + b_{10}\right) \\
\sigma\left(\vec{x_{2, n}}.\vec{w_{i, 1}} + b_1\right) &amp; \sigma\left(\vec{x_{2, n}}.\vec{w_{i, 2}} + b_2\right) &amp; \dots &amp; \sigma\left(\vec{x_{2, n}}.\vec{w_{i, 10}} + b_{10}\right) \\
\sigma\left(\vec{x_{3, n}}.\vec{w_{i, 1}} + b_1\right) &amp; \sigma\left(\vec{x_{3, n}}.\vec{w_{i, 2}} + b_2\right) &amp; \dots &amp; \sigma\left(\vec{x_{3, n}}.\vec{w_{i, 10}} + b_{10}\right) \\
\vdots &amp; \vdots &amp; \vdots &amp; \vdots \\
\sigma\left(\vec{x_{60000, n}}.\vec{w_{i, 1}} + b_1\right) &amp; \sigma\left(\vec{x_{60000, n}}.\vec{w_{i, 2}} + b_2\right) &amp; \dots &amp; \sigma\left(\vec{x_{60000, n}}.\vec{w_{i, 10}} + b_{10}\right) \\
\end{bmatrix}
\]</span></p>
<div id="973bf5db" class="cell" data-execution_count="2">
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> init_params():</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This function generate vectors of random weights and bias for each layer .</span><span class="ch">\n</span></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a><span class="co">    - w1 : the vector of weights between the input layer and the hidden layer.</span></span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a><span class="co">    - w2 : the vector of weights between the hidden layer and the output layer.</span></span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a><span class="co">    - b1 : the vector of biases for each neurons of the hidden layer.</span></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a><span class="co">    - b2 : the vector of biases for each neurons of the output layer.</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-8"><a href="#cb2-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-9"><a href="#cb2-9" aria-hidden="true" tabindex="-1"></a><span class="co">        tuple: a tuple, for easy unpacking, of vectors for each layers weigths and biases</span></span>
<span id="cb2-10"><a href="#cb2-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-11"><a href="#cb2-11" aria-hidden="true" tabindex="-1"></a>    w1 <span class="op">=</span> np.random.randn(<span class="dv">784</span>, <span class="dv">10</span>)</span>
<span id="cb2-12"><a href="#cb2-12" aria-hidden="true" tabindex="-1"></a>    w2 <span class="op">=</span> np.random.randn(<span class="dv">10</span>, <span class="dv">10</span>)</span>
<span id="cb2-13"><a href="#cb2-13" aria-hidden="true" tabindex="-1"></a>    b1 <span class="op">=</span> np.random.randn(<span class="dv">10</span>)</span>
<span id="cb2-14"><a href="#cb2-14" aria-hidden="true" tabindex="-1"></a>    b2 <span class="op">=</span> np.random.randn(<span class="dv">10</span>)</span>
<span id="cb2-15"><a href="#cb2-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> w1, w2, b1, b2</span>
<span id="cb2-16"><a href="#cb2-16" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-17"><a href="#cb2-17" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-18"><a href="#cb2-18" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> Z(x: np.array, w: np.array, b: np.array):</span>
<span id="cb2-19"><a href="#cb2-19" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""This function compute the hypothesis function for an entire layer. It also works for batch sampling</span></span>
<span id="cb2-20"><a href="#cb2-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-21"><a href="#cb2-21" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb2-22"><a href="#cb2-22" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.array): A vector or matrix. if matrix, each input should be row-wise, instead of column wise (e.g an image for one row, thus one array)</span></span>
<span id="cb2-23"><a href="#cb2-23" aria-hidden="true" tabindex="-1"></a><span class="co">        w (np.array): A matrix of weights for between all previous layer neurons (row) and all next layer neurons (col).</span></span>
<span id="cb2-24"><a href="#cb2-24" aria-hidden="true" tabindex="-1"></a><span class="co">        b (np.array): A vector of biais for neurons.</span></span>
<span id="cb2-25"><a href="#cb2-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-26"><a href="#cb2-26" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb2-27"><a href="#cb2-27" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array: A matrix containing the z's for the whole layer, be it the hidden or the output layer. Rows are the z's per input and columns are the different neurons.</span></span>
<span id="cb2-28"><a href="#cb2-28" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb2-29"><a href="#cb2-29" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-30"><a href="#cb2-30" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> x.shape[<span class="dv">1</span>] <span class="op">!=</span> w.shape[<span class="dv">0</span>]:</span>
<span id="cb2-31"><a href="#cb2-31" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb2-32"><a href="#cb2-32" aria-hidden="true" tabindex="-1"></a>                <span class="st">"X row length doesn't match the lenth of w. </span><span class="ch">\n</span><span class="st"> Don't forget that each features needs to be transposed row-wise, having an single array for each feature."</span></span>
<span id="cb2-33"><a href="#cb2-33" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb2-34"><a href="#cb2-34" aria-hidden="true" tabindex="-1"></a>    <span class="cf">finally</span>:</span>
<span id="cb2-35"><a href="#cb2-35" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"X and W format match!"</span>)</span>
<span id="cb2-36"><a href="#cb2-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">try</span>:</span>
<span id="cb2-37"><a href="#cb2-37" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> w.shape[<span class="dv">1</span>] <span class="op">!=</span> <span class="bu">len</span>(b):</span>
<span id="cb2-38"><a href="#cb2-38" aria-hidden="true" tabindex="-1"></a>            <span class="cf">raise</span> <span class="pp">ValueError</span>(</span>
<span id="cb2-39"><a href="#cb2-39" aria-hidden="true" tabindex="-1"></a>                <span class="st">"W columns length doesn't match the length of b. </span><span class="ch">\n</span><span class="st"> W should have one column for each neurons of the tested layer."</span></span>
<span id="cb2-40"><a href="#cb2-40" aria-hidden="true" tabindex="-1"></a>            )</span>
<span id="cb2-41"><a href="#cb2-41" aria-hidden="true" tabindex="-1"></a>    <span class="cf">finally</span>:</span>
<span id="cb2-42"><a href="#cb2-42" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="st">"W and B format match!"</span>)</span>
<span id="cb2-43"><a href="#cb2-43" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-44"><a href="#cb2-44" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> x.dot(w) <span class="op">+</span> b</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="relu" class="level3">
<h3 class="anchored" data-anchor-id="relu">ReLU</h3>
<p>For the hidden layer, I’ll chose the ReLU activation function, since it’s a good default choice. It also optimize sparsity in the model by outputting 0 for each each negative value. This behavior reduces useless noise of make the model more aware of the important feature.</p>
<p><span class="math display">\[
a = \text{ReLU}(z) = \max(0, z)
\]</span></p>
<p>Where: * <span class="math inline">\(a\)</span>: Output of the ReLU activation function for a given input <span class="math inline">\(z\)</span>. * <span class="math inline">\(z\)</span>: Input to the activation function (typically the result of <span class="math inline">\(Wx + b\)</span>). * <span class="math inline">\(\text{ReLU}(z)\)</span>: The Rectified Linear Unit activation function, which outputs <span class="math inline">\(z\)</span> if <span class="math inline">\(z &gt; 0\)</span>, and <span class="math inline">\(0\)</span> otherwise.</p>
<div id="afc97419" class="cell" data-execution_count="3">
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ReLU(z):</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Generate the maximum of z element-wise, thus returning 0 if z &lt;= 0, and z if z &gt; 0. </span><span class="ch">\n</span></span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a><span class="co">        z (np.array): A vector or matrix of all z score for all neurons.</span></span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array : a vector or matrix, depending on z's format : 0 if z &lt;= 0, and z if z &gt; 0.</span></span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.maximum(<span class="dv">0</span>, z)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="softmax" class="level3">
<h3 class="anchored" data-anchor-id="softmax">Softmax</h3>
<p>For the activation function of the output layer, I’ll chose the softmax() function, since we’ll need probabilities as outputs. <span class="math display">\[
a_i = \text{softmax}(z_i) = \frac{e^{z_i}}{\sum_{j=1}^N e^{z_j}}
\]</span></p>
<p>Where: * <span class="math inline">\(a_i\)</span>: Output of the softmax function for the <span class="math inline">\(i\)</span>-th class. * <span class="math inline">\(z_i\)</span>: Input to the softmax function for the <span class="math inline">\(i\)</span>-th class (typically the result of <span class="math inline">\(Wx + b\)</span> for class <span class="math inline">\(i\)</span>). * <span class="math inline">\(N\)</span>: Total number of classes (outputs). * <span class="math inline">\(\text{softmax}(z_i)\)</span>: Computes the normalized probability distribution across all classes. * <span class="math inline">\(\sum_{j=1}^N e^{z_j}\)</span>: Normalization term ensuring that the outputs sum to 1.</p>
<p>The softmax function is commonly used in the output layer of a neural network for classification problems where the goal is to predict probabilities for multiple classes.</p>
<p>However, the usual softmax function can lead to two types of problems : * Overlfow: it occurs when number is approximated as infinity. * Underflow: it occurs when a numer is very small and approximated to 0.</p>
<p>In order to prevent this from happening, I’ll compute the stable form of the softmax function. This stable form solves both problems by scaling the exponent to a smaller number. In our case, I’ll normalized the z values, so that they are neither too big, nor too small.</p>
<p><span class="math display">\[
a_i = \text{softmax}(z_i) = \frac{e^{\frac{z_i - \mu}{\sigma}}}{\sum_{j=1}^N e^{\frac{z_j - \mu}{\sigma}}}
\]</span></p>
<p>Where: * <span class="math inline">\(a_i\)</span>: Output of the softmax function for the <span class="math inline">\(i\)</span>-th class. * <span class="math inline">\(z_i\)</span>: Input to the softmax function for the <span class="math inline">\(i\)</span>-th class (typically the result of <span class="math inline">\(Wx + b\)</span> for class <span class="math inline">\(i\)</span>). * <span class="math inline">\(N\)</span>: Total number of classes (outputs). * <span class="math inline">\(\mu\)</span>: The mean of the input values <span class="math inline">\(\mathbf{z}\)</span>, i.e., <span class="math inline">\(\mu = \frac{1}{N} \sum_{j=1}^N z_j\)</span>. * <span class="math inline">\(\sigma\)</span>: The standard deviation of the input values <span class="math inline">\(\mathbf{z}\)</span>, i.e., <span class="math inline">\(\sigma = \sqrt{\frac{1}{N} \sum_{j=1}^N (z_j - \mu)^2}\)</span>. * <span class="math inline">\(\text{softmax}(z_i)\)</span>: Computes the normalized probability distribution across all classes. * <span class="math inline">\(\sum_{j=1}^N e^{\frac{z_j - \mu}{\sigma}}\)</span>: Normalization term ensuring that the outputs sum to 1.</p>
<div id="78523b34" class="cell" data-execution_count="4">
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> stand_softmax(z):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""Transforming an array of numerical value into their probability using the standardized softmax equation. </span><span class="ch">\n</span></span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Exponent are shifted by substracting the maximum of all z scores to each element-wise z.</span></span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a><span class="co">        z (np.array): A array of all z score for all neurons. </span><span class="ch">\n</span></span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a><span class="co">        Columns are a single neurons z, while rows are inputs.</span></span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array : np.array of softmax scores in the same format as z.</span></span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>    stand_z <span class="op">=</span> (np.mean(z, axis<span class="op">=</span><span class="dv">0</span>) <span class="op">-</span> z) <span class="op">/</span> np.std(</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>        z, axis<span class="op">=</span><span class="dv">0</span></span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>    )  <span class="co"># each column of z needs to be a neurons result.</span></span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.exp(stand_z) <span class="op">/</span> np.<span class="bu">sum</span>(np.exp(stand_z), axis<span class="op">=</span><span class="dv">0</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
<section id="forward-pass" class="level3">
<h3 class="anchored" data-anchor-id="forward-pass">Forward pass</h3>
<p>We now have all the necessary building blocks for computing the forward pass, meaning half of the complete cycle that the model would do, the rest being ‘backward propagation’. This is where the model learns from its outputs the correct solution to module its weights to do better next time.</p>
<div id="04d3d06c" class="cell" data-execution_count="5">
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> forward_pass(X, w1, w2, b1, b2):</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""The function computing the hypothesis function the hidden layer and the output layer, thus giving out the model's result</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-4"><a href="#cb5-4" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb5-5"><a href="#cb5-5" aria-hidden="true" tabindex="-1"></a><span class="co">        x (np.array): A vector containing the row the model should learn</span></span>
<span id="cb5-6"><a href="#cb5-6" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-7"><a href="#cb5-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb5-8"><a href="#cb5-8" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array: a vector containing the output of each neurons of the output layer. Thus, this vector has the shape(10, 1).</span></span>
<span id="cb5-9"><a href="#cb5-9" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb5-10"><a href="#cb5-10" aria-hidden="true" tabindex="-1"></a>    z1 <span class="op">=</span> Z(X, w1, b1)</span>
<span id="cb5-11"><a href="#cb5-11" aria-hidden="true" tabindex="-1"></a>    a1 <span class="op">=</span> ReLU(z1)</span>
<span id="cb5-12"><a href="#cb5-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-13"><a href="#cb5-13" aria-hidden="true" tabindex="-1"></a>    z2 <span class="op">=</span> Z(a1, w2, b2)</span>
<span id="cb5-14"><a href="#cb5-14" aria-hidden="true" tabindex="-1"></a>    a2 <span class="op">=</span> stand_softmax(z2)</span>
<span id="cb5-15"><a href="#cb5-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb5-16"><a href="#cb5-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> z1, a1, z2, a2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="backward-propagation" class="level2">
<h2 class="anchored" data-anchor-id="backward-propagation">Backward Propagation</h2>
<section id="weight-updating" class="level3">
<h3 class="anchored" data-anchor-id="weight-updating">Weight updating</h3>
<p>Now that our model can compute its score using random weights, it needs to learn to adjust its weight according to its performance. This process is called backward propagation, where the model will modify its weights in order to minimize a defined loss function.</p>
<p>One way to do this is to look at the partial derivative of the loss function with respect to the weight parameter <span class="math inline">\(\frac{\partial L}{\partial W}\)</span>. Since we have weights before and after the hidden layer, we’ll need two different equations. This will give us the direction that takes the loss function when adujsting the weight, by keeping all other variable constant. It’s like knowing the slop of the loss function. By knowing if the loss function increase of decrease the higher the weight are, we can adjust them to minimze it.</p>
<p><span class="math inline">\(\frac{\partial L}{\partial W_1}\)</span> and <span class="math inline">\(\frac{\partial L}{\partial W_2}\)</span> are both computed by finding how the weights in the hidden layer and the output layer propagates their influences all the way to the results <span class="math inline">\(\hat{y}_i\)</span>. This influence is called the error term <span class="math inline">\(\delta\)</span>. In our case, there is <span class="math inline">\(\delta_1\)</span> for the hidden layer’s weights’ influence on the result, and <span class="math inline">\(\delta_2\)</span> for the output’s layer weights’ influence on the result. Both error terms originates from the input that the neurons of either layer were fed, either <span class="math inline">\(X\)</span> or <span class="math inline">\(\hat{y}_1\)</span>.</p>
<p>This gives us : <span class="math display">\[
\frac{\partial L}{\partial W_2} = \delta_2^{T} \cdot \hat{y}_1
\]</span> <span class="math display">\[
\frac{\partial L}{\partial W_1} = \delta_1^{T} \cdot X
\]</span> Additionnaly, by undwinding the model computations and using the chain rule, we can define both error terms further. The term <span class="math inline">\(\delta_1\)</span> incorporates the error signal from the subsequent layer <span class="math inline">\(\left(\delta_2\right)\)</span> and adjusts it for each neuron in the hidden layer using the derivative of the ReLU activation function. <span class="math display">\[
\delta_2 = \frac{\partial L}{\partial \hat{y}_2} \odot \frac{\partial
\hat{y}_2}{\partial Z_2}
\]</span> <span class="math display">\[
\delta_1 = \left(W_2 \cdot \delta_2^{T}\right)^{T} \odot \frac{\partial \hat{y}_1}
{\partial Z_1}
\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\frac{\partial L}{\partial \hat{y}_2} = -2 \odot (y_i - \hat{y}_2)\)</span></p>
<p><span class="math inline">\(\frac{\partial \hat{y}_2}{\partial Z_2} = softmax(Z_2) \odot \left(1 - softmax
(Z_2)\right)\)</span></p>
<p><span class="math inline">\(\frac{\partial \hat{y}_1}{\partial Z_1} =
\frac{\partial \text{ReLU}(Z1)}{\partial Z1} =
\begin{cases}
0, &amp; \text{if } Z1 &lt; 0 \\
1, &amp; \text{if } Z1 \geq 0
\end{cases}\)</span></p>
<p>Then, once both gradients of loss with respect to the weights <span class="math inline">\(\frac{\partial L}{\partial W}\)</span> are computed, we can proceed updating them. We update them by substracting the orginal weights to the gradient of loss found. This will nudge slightly the weight in the correct direction.</p>
<p><span class="math display">\[
W_{new} = W_{old} - \eta \odot \left(\frac{\partial L}{\partial W}\right)^{T}
\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\eta\)</span> is the learning rate.</p>
</section>
<section id="bias-updating" class="level3">
<h3 class="anchored" data-anchor-id="bias-updating">Bias updating</h3>
<p>Now that our weight have been updated, we also need to update the biaises. Biases are updated them same way weights are.</p>
<p><span class="math display">\[
\frac{\partial L}{\partial b_2} = \sum_{i = 1}^{N} \delta_2 = \sum_{i = 1}^{N}\left(\frac{\partial L}{\partial \hat{y}_2} \odot \frac{\partial \hat{y}_2}{\partial Z_2} \odot \frac{\partial Z_2}{\partial b_2}\right)
\]</span></p>
<p>Where:</p>
<p><span class="math inline">\(\frac{\partial Z_2}{\partial b_2} = 1\)</span></p>
<p><span class="math inline">\(N\)</span> is the number of input or example fed to the model. It is often represented by the numbers of row in the original dataset.</p>
<p><span class="math display">\[
\frac{\partial L}{\partial b_1} = \sum_{i = 1}^{N} \delta_1 = \sum_{i = 1}^{N}\left(\left(W_2 \cdot \delta_2^{T}\right)^{T} \odot \frac{\partial \hat{y}_1}{\partial Z_1} \odot \frac{\partial Z_1}{\partial b_1}\right)
\]</span></p>
<p>Where:</p>
<p>$ = 1$</p>
<p><span class="math inline">\(N\)</span> is the number of input or example fed to the model.</p>
<p>The updating rule is the same for both weights and biases.</p>
<p><span class="math display">\[
b_{new} = b_{old} - \eta \odot \frac{\partial L}{\partial b}
\]</span></p>
<div id="2b43baec" class="cell" data-execution_count="6">
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> ReLU_deriv(z):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> np.greater(z, <span class="dv">0</span>)</span>
<span id="cb6-3"><a href="#cb6-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-4"><a href="#cb6-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-5"><a href="#cb6-5" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> one_hot(Y, nunits_output: <span class="bu">int</span>):</span>
<span id="cb6-6"><a href="#cb6-6" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""A function that one-hot encode the y scalar value. </span><span class="ch">\n</span></span>
<span id="cb6-7"><a href="#cb6-7" aria-hidden="true" tabindex="-1"></a><span class="co">    Meaning, it expend it to match the number of units in the output layer. </span><span class="ch">\n</span></span>
<span id="cb6-8"><a href="#cb6-8" aria-hidden="true" tabindex="-1"></a><span class="co">    The position of the scalar value inside the array is the same as its value.</span><span class="ch">\n</span></span>
<span id="cb6-9"><a href="#cb6-9" aria-hidden="true" tabindex="-1"></a><span class="co">    This way, the 5th neurons of the output layer should respond the number 5 the most.</span></span>
<span id="cb6-10"><a href="#cb6-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-11"><a href="#cb6-11" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb6-12"><a href="#cb6-12" aria-hidden="true" tabindex="-1"></a><span class="co">        y (int): scalar value representing the right solutiont that the model should guess.</span></span>
<span id="cb6-13"><a href="#cb6-13" aria-hidden="true" tabindex="-1"></a><span class="co">        n_units (int): The numbers of neurons inside the output layer. </span><span class="ch">\n</span></span>
<span id="cb6-14"><a href="#cb6-14" aria-hidden="true" tabindex="-1"></a><span class="co">        The position of the scalar value inside the array is the same as its value.</span></span>
<span id="cb6-15"><a href="#cb6-15" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-16"><a href="#cb6-16" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb6-17"><a href="#cb6-17" aria-hidden="true" tabindex="-1"></a><span class="co">        np.array: A np.array, extension of y.</span></span>
<span id="cb6-18"><a href="#cb6-18" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb6-19"><a href="#cb6-19" aria-hidden="true" tabindex="-1"></a>    y_cold <span class="op">=</span> []</span>
<span id="cb6-20"><a href="#cb6-20" aria-hidden="true" tabindex="-1"></a>    array <span class="op">=</span> [<span class="dv">0</span>] <span class="op">*</span> nunits_output</span>
<span id="cb6-21"><a href="#cb6-21" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> res <span class="kw">in</span> Y:</span>
<span id="cb6-22"><a href="#cb6-22" aria-hidden="true" tabindex="-1"></a>        copy <span class="op">=</span> array.copy()</span>
<span id="cb6-23"><a href="#cb6-23" aria-hidden="true" tabindex="-1"></a>        copy[res] <span class="op">=</span> <span class="dv">1</span></span>
<span id="cb6-24"><a href="#cb6-24" aria-hidden="true" tabindex="-1"></a>        y_cold.append(copy)</span>
<span id="cb6-25"><a href="#cb6-25" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-26"><a href="#cb6-26" aria-hidden="true" tabindex="-1"></a>    y_hot <span class="op">=</span> np.array(y_cold)</span>
<span id="cb6-27"><a href="#cb6-27" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> y_hot</span>
<span id="cb6-28"><a href="#cb6-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-29"><a href="#cb6-29" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-30"><a href="#cb6-30" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> update_rule(w1, w2, b1, b2, dL_dw1, dL_dw2, dL_db1, dL_db2, eta):</span>
<span id="cb6-31"><a href="#cb6-31" aria-hidden="true" tabindex="-1"></a>    new_w1 <span class="op">=</span> w1 <span class="op">-</span> (eta <span class="op">*</span> dL_dw1.T)</span>
<span id="cb6-32"><a href="#cb6-32" aria-hidden="true" tabindex="-1"></a>    new_w2 <span class="op">=</span> w2 <span class="op">-</span> (eta <span class="op">*</span> dL_dw2.T)</span>
<span id="cb6-33"><a href="#cb6-33" aria-hidden="true" tabindex="-1"></a>    new_b1 <span class="op">=</span> b1 <span class="op">-</span> (eta <span class="op">*</span> dL_db1)</span>
<span id="cb6-34"><a href="#cb6-34" aria-hidden="true" tabindex="-1"></a>    new_b2 <span class="op">=</span> b2 <span class="op">-</span> (eta <span class="op">*</span> dL_db2)</span>
<span id="cb6-35"><a href="#cb6-35" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-36"><a href="#cb6-36" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> new_w1, new_w2, new_b1, new_b2</span>
<span id="cb6-37"><a href="#cb6-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-38"><a href="#cb6-38" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-39"><a href="#cb6-39" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> backward_prop(X, Y, w2, z1, z2, a1, n_units):</span>
<span id="cb6-40"><a href="#cb6-40" aria-hidden="true" tabindex="-1"></a>    y <span class="op">=</span> one_hot(Y, n_units)</span>
<span id="cb6-41"><a href="#cb6-41" aria-hidden="true" tabindex="-1"></a>    y2_hat <span class="op">=</span> stand_softmax(z2)</span>
<span id="cb6-42"><a href="#cb6-42" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-43"><a href="#cb6-43" aria-hidden="true" tabindex="-1"></a>    dL_dy2 <span class="op">=</span> <span class="op">-</span><span class="dv">2</span> <span class="op">*</span> (y <span class="op">-</span> y2_hat)</span>
<span id="cb6-44"><a href="#cb6-44" aria-hidden="true" tabindex="-1"></a>    dy2_dz2 <span class="op">=</span> stand_softmax(z2) <span class="op">*</span> (<span class="dv">1</span> <span class="op">-</span> stand_softmax(z2))</span>
<span id="cb6-45"><a href="#cb6-45" aria-hidden="true" tabindex="-1"></a>    dy1__dZ1 <span class="op">=</span> ReLU_deriv(z1)</span>
<span id="cb6-46"><a href="#cb6-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-47"><a href="#cb6-47" aria-hidden="true" tabindex="-1"></a>    w_error2 <span class="op">=</span> dL_dy2 <span class="op">*</span> dy2_dz2</span>
<span id="cb6-48"><a href="#cb6-48" aria-hidden="true" tabindex="-1"></a>    w_error1 <span class="op">=</span> w2.dot(w_error2.T).T <span class="op">*</span> dy1__dZ1</span>
<span id="cb6-49"><a href="#cb6-49" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-50"><a href="#cb6-50" aria-hidden="true" tabindex="-1"></a>    dL_dw1 <span class="op">=</span> w_error1.T.dot(X)</span>
<span id="cb6-51"><a href="#cb6-51" aria-hidden="true" tabindex="-1"></a>    dL_dw2 <span class="op">=</span> w_error2.T.dot(a1)</span>
<span id="cb6-52"><a href="#cb6-52" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-53"><a href="#cb6-53" aria-hidden="true" tabindex="-1"></a>    dL_db2 <span class="op">=</span> np.mean(w_error2, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-54"><a href="#cb6-54" aria-hidden="true" tabindex="-1"></a>    dL_db1 <span class="op">=</span> np.mean(w_error1, axis<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb6-55"><a href="#cb6-55" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb6-56"><a href="#cb6-56" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> dL_dw1, dL_dw2, dL_db1, dL_db2</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
</section>
</section>
<section id="prediction" class="level2">
<h2 class="anchored" data-anchor-id="prediction">Prediction</h2>
<p>The model have been successfully created. The last step is to train and test it. For the training, I’ll use the whole training dataset, which have a shape of <span class="math inline">\(\left(60000, 784\right)\)</span>. For the testing, I’ll use the whole training dataset, which have slightly less images than the training dataset # <span class="math inline">\(\left(10000, 784\right)\)</span>. Normally, only using a training and a testing set as I’m doing isn’t recommended. This is because, one will fine tune the model based on the testing set, since this is the set that will be used for performance measurement. However, in our case, we won’t fine tune the model to improve its performance. Additionnaly, in order to prevent for information losses, cross-validation is also recommended. However, with this project, my purpose isn’t to make the best model possible (otherwise, I would have used more hidden layers and maybe more adapted adaptation functions). Consequently, cross-validation won’t be used.</p>
<p>Concerning decerning the model’s performance, the accuracy and f1 score will be used. Accuracy doesn’t provide information about False Positives or False Negative, which the F1 score does. Additionnaly, the F1 Score is computed using the harmonic mean of the recall and precision rate, which makes it a more stricter score. Indeed, this is because low recall or precision rates have a high negative impact on the final score.</p>
<p><span class="math display">\[
F1 Score : \frac{2 * Recall * Precision}{Recall + Precision}
\]</span></p>
<p>Where:</p>
<ul>
<li><p><span class="math inline">\(Recall = \frac{True Positives}{True Positives + False Negatives}\)</span></p></li>
<li><p><span class="math inline">\(Precision = \frac{True Positives}{True Positives + False Positives}\)</span></p></li>
</ul>
<section id="training-performance" class="level3">
<h3 class="anchored" data-anchor-id="training-performance">Training performance</h3>
<div id="56b57785" class="cell">
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> get_scores(Y, a2, n_iter, n_unit<span class="op">=</span><span class="dv">10</span>):</span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a>    data <span class="op">=</span> []</span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a>    pred_y_df <span class="op">=</span> pd.DataFrame(</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>        data<span class="op">=</span>{<span class="st">"Predictions"</span>: np.argmax(a2, axis<span class="op">=</span><span class="dv">1</span>), <span class="st">"True Value"</span>: Y}</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> num <span class="kw">in</span> <span class="bu">list</span>(<span class="bu">range</span>(<span class="dv">0</span>, n_unit, <span class="dv">1</span>)):</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a>        pred_num_df <span class="op">=</span> pred_y_df.loc[pred_y_df[<span class="st">"True Value"</span>] <span class="op">==</span> num]</span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        pred_notnum_df <span class="op">=</span> pred_y_df.loc[pred_y_df[<span class="st">"True Value"</span>] <span class="op">!=</span> num]</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a>        tp <span class="op">=</span> (pred_num_df[<span class="st">"Predictions"</span>] <span class="op">==</span> pred_num_df[<span class="st">"True Value"</span>]).<span class="bu">sum</span>()</span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        fn <span class="op">=</span> (pred_num_df[<span class="st">"Predictions"</span>] <span class="op">!=</span> pred_num_df[<span class="st">"True Value"</span>]).<span class="bu">sum</span>()</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        fp <span class="op">=</span> (pred_notnum_df[<span class="st">"Predictions"</span>] <span class="op">==</span> num).<span class="bu">sum</span>()</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        tn <span class="op">=</span> (pred_notnum_df[<span class="st">"Predictions"</span>] <span class="op">!=</span> num).<span class="bu">sum</span>()</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>        accuracy <span class="op">=</span> (tp <span class="op">/</span> <span class="bu">len</span>(pred_num_df.index)) <span class="cf">if</span> <span class="bu">len</span>(pred_num_df.index) <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        precision <span class="op">=</span> (tp <span class="op">/</span> (tp <span class="op">+</span> fp)) <span class="cf">if</span> tp <span class="op">+</span> fp <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>        recall <span class="op">=</span> (tp <span class="op">/</span> (tp <span class="op">+</span> fn)) <span class="cf">if</span> tp <span class="op">+</span> fn <span class="op">&gt;</span> <span class="dv">0</span> <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>        f1_score <span class="op">=</span> (</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>            (<span class="dv">2</span> <span class="op">*</span> precision <span class="op">*</span> recall) <span class="op">/</span> (precision <span class="op">+</span> recall)</span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>            <span class="cf">if</span> (precision <span class="op">+</span> recall) <span class="op">&gt;</span> <span class="dv">0</span></span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">else</span> <span class="dv">0</span></span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        data.append(</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a>            [</span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>                n_iter,</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>                num,</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>                tp,</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>                fp,</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>                tn,</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>                fn,</span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>                precision,</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>                recall,</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>                accuracy,</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>                f1_score,</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>            ]</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pd.DataFrame(</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        data,</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        columns<span class="op">=</span>[</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Iteration"</span>,</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>            <span class="st">"True Class"</span>,</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>            <span class="st">"TP"</span>,</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>            <span class="st">"FP"</span>,</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>            <span class="st">"TN"</span>,</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a>            <span class="st">"FN"</span>,</span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Precision"</span>,</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Recall"</span>,</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>            <span class="st">"Accuracy"</span>,</span>
<span id="cb7-50"><a href="#cb7-50" aria-hidden="true" tabindex="-1"></a>            <span class="st">"F1-Score"</span>,</span>
<span id="cb7-51"><a href="#cb7-51" aria-hidden="true" tabindex="-1"></a>        ],</span>
<span id="cb7-52"><a href="#cb7-52" aria-hidden="true" tabindex="-1"></a>    )</span>
<span id="cb7-53"><a href="#cb7-53" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-54"><a href="#cb7-54" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-55"><a href="#cb7-55" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> pred(X, Y, n_iter, eta, params<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb7-56"><a href="#cb7-56" aria-hidden="true" tabindex="-1"></a>    perf <span class="op">=</span> []</span>
<span id="cb7-57"><a href="#cb7-57" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-58"><a href="#cb7-58" aria-hidden="true" tabindex="-1"></a>    <span class="cf">if</span> params <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb7-59"><a href="#cb7-59" aria-hidden="true" tabindex="-1"></a>        w1, w2, b1, b2 <span class="op">=</span> init_params()</span>
<span id="cb7-60"><a href="#cb7-60" aria-hidden="true" tabindex="-1"></a>    <span class="cf">else</span>:</span>
<span id="cb7-61"><a href="#cb7-61" aria-hidden="true" tabindex="-1"></a>        w1, w2, b1, b2 <span class="op">=</span> params</span>
<span id="cb7-62"><a href="#cb7-62" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-63"><a href="#cb7-63" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> i <span class="kw">in</span> <span class="bu">range</span>(<span class="dv">0</span>, n_iter, <span class="dv">1</span>):</span>
<span id="cb7-64"><a href="#cb7-64" aria-hidden="true" tabindex="-1"></a>        z1, a1, z2, a2 <span class="op">=</span> forward_pass(X, w1, w2, b1, b2)</span>
<span id="cb7-65"><a href="#cb7-65" aria-hidden="true" tabindex="-1"></a>        dL_dw1, dL_dw2, dL_db1, dL_db2 <span class="op">=</span> backward_prop(X, Y, w2, z1, z2, a1, n_units<span class="op">=</span><span class="dv">10</span>)</span>
<span id="cb7-66"><a href="#cb7-66" aria-hidden="true" tabindex="-1"></a>        w1, w2, b1, b2 <span class="op">=</span> update_rule(</span>
<span id="cb7-67"><a href="#cb7-67" aria-hidden="true" tabindex="-1"></a>            w1, w2, b1, b2, dL_dw1, dL_dw2, dL_db1, dL_db2, eta</span>
<span id="cb7-68"><a href="#cb7-68" aria-hidden="true" tabindex="-1"></a>        )</span>
<span id="cb7-69"><a href="#cb7-69" aria-hidden="true" tabindex="-1"></a>        perf.append(get_scores(Y, a2, n_iter<span class="op">=</span>i))</span>
<span id="cb7-70"><a href="#cb7-70" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-71"><a href="#cb7-71" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> (pd.concat(perf, axis<span class="op">=</span><span class="dv">0</span>), (w1, w2, b1, b2))</span>
<span id="cb7-72"><a href="#cb7-72" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-73"><a href="#cb7-73" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-74"><a href="#cb7-74" aria-hidden="true" tabindex="-1"></a>score, parameter <span class="op">=</span> pred(X_train_stand, Y_train, n_iter<span class="op">=</span><span class="dv">500</span>, eta<span class="op">=</span><span class="fl">0.1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>As you can see from the result below, there is a clear difference between maximum accuracy and the mean accuracy. This means that accuracy of most numbers are low, except for a few chosen one having expceptionnaly good detection rate compared to the others. Additionnaly, past the 200th iteration, the model’s performance didn’t improved but kept oscillating. More importantly, even the class scoring the best accuracy have a pityfully high rate of False Positives. Those performances aren’t that surprising considering that our model only has a single hidden layer of 10 units.</p>
<p>To conclude, given more depth, we can see a path of improvement for this model.</p>
<div id="fcf8fb2a" class="cell" data-execution_count="8">
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>mean_acc <span class="op">=</span> score.groupby(<span class="st">"Iteration"</span>)[<span class="st">"Accuracy"</span>].mean()</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">500</span>)</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> mean_acc</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Mean Accuracy"</span>)</span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training data, development of mean accuracy"</span>)</span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>plt.show()</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>max_acc <span class="op">=</span> score.groupby(<span class="st">"Iteration"</span>)[<span class="st">"Accuracy"</span>].<span class="bu">max</span>()</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>x <span class="op">=</span> np.arange(<span class="dv">500</span>)</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>y <span class="op">=</span> max_acc</span>
<span id="cb8-13"><a href="#cb8-13" aria-hidden="true" tabindex="-1"></a>plt.plot(x, y)</span>
<span id="cb8-14"><a href="#cb8-14" aria-hidden="true" tabindex="-1"></a>plt.xlabel(<span class="st">"Iterations"</span>)</span>
<span id="cb8-15"><a href="#cb8-15" aria-hidden="true" tabindex="-1"></a>plt.ylabel(<span class="st">"Maximum Accuracy"</span>)</span>
<span id="cb8-16"><a href="#cb8-16" aria-hidden="true" tabindex="-1"></a>plt.title(<span class="st">"Training data, development of maximum accuracy"</span>)</span>
<span id="cb8-17"><a href="#cb8-17" aria-hidden="true" tabindex="-1"></a>plt.show()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="script_files/figure-html/cell-9-output-1.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
<div class="cell-output cell-output-display">
<div>
<figure class="figure">
<p><img src="script_files/figure-html/cell-9-output-2.png" class="img-fluid figure-img"></p>
</figure>
</div>
</div>
</div>
<div id="0ae1929e" class="cell" data-execution_count="9">
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Performance Scores for the last iteration of the model </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb9-2"><a href="#cb9-2" aria-hidden="true" tabindex="-1"></a>score.loc[score[<span class="st">"Iteration"</span>] <span class="op">==</span> <span class="dv">499</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Performance Scores for the last iteration of the model 
</code></pre>
</div>
<div class="cell-output cell-output-display" data-execution_count="9">
<div>


<table class="dataframe caption-top table table-sm table-striped small" data-quarto-postprocess="true" data-border="1">
<thead>
<tr class="header">
<th data-quarto-table-cell-role="th"></th>
<th data-quarto-table-cell-role="th">Iteration</th>
<th data-quarto-table-cell-role="th">True Class</th>
<th data-quarto-table-cell-role="th">TP</th>
<th data-quarto-table-cell-role="th">FP</th>
<th data-quarto-table-cell-role="th">TN</th>
<th data-quarto-table-cell-role="th">FN</th>
<th data-quarto-table-cell-role="th">Precision</th>
<th data-quarto-table-cell-role="th">Recall</th>
<th data-quarto-table-cell-role="th">Accuracy</th>
<th data-quarto-table-cell-role="th">F1-Score</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td data-quarto-table-cell-role="th">0</td>
<td>499</td>
<td>0</td>
<td>54</td>
<td>716</td>
<td>53361</td>
<td>5869</td>
<td>0.070130</td>
<td>0.009117</td>
<td>0.009117</td>
<td>0.016136</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">1</td>
<td>499</td>
<td>1</td>
<td>327</td>
<td>1893</td>
<td>51365</td>
<td>6415</td>
<td>0.147297</td>
<td>0.048502</td>
<td>0.048502</td>
<td>0.072975</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">2</td>
<td>499</td>
<td>2</td>
<td>0</td>
<td>0</td>
<td>54042</td>
<td>5958</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">3</td>
<td>499</td>
<td>3</td>
<td>988</td>
<td>11249</td>
<td>42620</td>
<td>5143</td>
<td>0.080739</td>
<td>0.161148</td>
<td>0.161148</td>
<td>0.107578</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">4</td>
<td>499</td>
<td>4</td>
<td>2586</td>
<td>28815</td>
<td>25343</td>
<td>3256</td>
<td>0.082354</td>
<td>0.442657</td>
<td>0.442657</td>
<td>0.138872</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">5</td>
<td>499</td>
<td>5</td>
<td>0</td>
<td>6</td>
<td>54573</td>
<td>5421</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">6</td>
<td>499</td>
<td>6</td>
<td>707</td>
<td>5726</td>
<td>48356</td>
<td>5211</td>
<td>0.109902</td>
<td>0.119466</td>
<td>0.119466</td>
<td>0.114485</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">7</td>
<td>499</td>
<td>7</td>
<td>998</td>
<td>5897</td>
<td>47838</td>
<td>5267</td>
<td>0.144743</td>
<td>0.159298</td>
<td>0.159298</td>
<td>0.151672</td>
</tr>
<tr class="odd">
<td data-quarto-table-cell-role="th">8</td>
<td>499</td>
<td>8</td>
<td>0</td>
<td>2</td>
<td>54147</td>
<td>5851</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
<td>0.000000</td>
</tr>
<tr class="even">
<td data-quarto-table-cell-role="th">9</td>
<td>499</td>
<td>9</td>
<td>4</td>
<td>32</td>
<td>54019</td>
<td>5945</td>
<td>0.111111</td>
<td>0.000672</td>
<td>0.000672</td>
<td>0.001337</td>
</tr>
</tbody>
</table>

</div>
</div>
</div>
</section>
<section id="testing-performance" class="level3">
<h3 class="anchored" data-anchor-id="testing-performance">Testing Performance</h3>
<div id="3c23f3a9" class="cell">
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>score_testing, parameter_testing <span class="op">=</span> pred(</span>
<span id="cb11-2"><a href="#cb11-2" aria-hidden="true" tabindex="-1"></a>    X_test_stand, Y_test, n_iter<span class="op">=</span><span class="dv">1</span>, eta<span class="op">=</span><span class="fl">0.1</span>, params<span class="op">=</span>parameter</span>
<span id="cb11-3"><a href="#cb11-3" aria-hidden="true" tabindex="-1"></a>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div>
<p>The training and testing mean accuracy is about the same. However, one can note that, although the maximum accuracy isn’t as good, its performances are more homogenous compared to the training set. Precision is also higher on average on the testing set. False Positive rate are also lower in the testing set, which supports the previous findings.</p>
<p>Although the model seems to respond well to the testing data, its performance are still overall disappointing. Adjustement of the learninig rate could bring out small benefits. However, to see significant improvements, adding more depth to the model is required.</p>
<div id="6020aec4" class="cell" data-execution_count="11">
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Score of the model on the testing set: </span><span class="ch">\n</span><span class="st">"</span>)</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(score_testing)</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Mean Accuracy: "</span>, score_testing[<span class="st">"Accuracy"</span>].mean())</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a><span class="bu">print</span>(<span class="st">"Maximum Accuracy: "</span>, score_testing[<span class="st">"Accuracy"</span>].<span class="bu">max</span>())</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<div class="cell-output cell-output-stdout">
<pre><code>Score of the model on the testing set: 

   Iteration  True Class   TP    FP    TN    FN  Precision    Recall  \
0          0           0    2    12  9008   978   0.142857  0.002041   
1          0           1    0     5  8860  1135   0.000000  0.000000   
2          0           2    0     1  8967  1032   0.000000  0.000000   
3          0           3   29   434  8556   981   0.062635  0.028713   
4          0           4    0     0  9018   982   0.000000  0.000000   
5          0           5  438  4895  4213   454   0.082130  0.491031   
6          0           6  296  3256  5786   662   0.083333  0.308977   
7          0           7    1     0  8972  1027   1.000000  0.000973   
8          0           8   18    93  8933   956   0.162162  0.018480   
9          0           9   67   453  8538   942   0.128846  0.066402   

   Accuracy  F1-Score  
0  0.002041  0.004024  
1  0.000000  0.000000  
2  0.000000  0.000000  
3  0.028713  0.039375  
4  0.000000  0.000000  
5  0.491031  0.140723  
6  0.308977  0.131264  
7  0.000973  0.001944  
8  0.018480  0.033180  
9  0.066402  0.087639  
Mean Accuracy:  0.0916617747290516
Maximum Accuracy:  0.4910313901345291</code></pre>
</div>
</div>
</section>
</section>

</main>
<!-- /main column -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const onCopySuccess = function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  }
  const getTextToCopy = function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button:not([data-in-quarto-modal])', {
    text: getTextToCopy
  });
  clipboard.on('success', onCopySuccess);
  if (window.document.getElementById('quarto-embedded-source-code-modal')) {
    // For code content inside modals, clipBoardJS needs to be initialized with a container option
    // TODO: Check when it could be a function (https://github.com/zenorocha/clipboard.js/issues/860)
    const clipboardModal = new window.ClipboardJS('.code-copy-button[data-in-quarto-modal]', {
      text: getTextToCopy,
      container: window.document.getElementById('quarto-embedded-source-code-modal')
    });
    clipboardModal.on('success', onCopySuccess);
  }
    var localhostRegex = new RegExp(/^(?:http|https):\/\/localhost\:?[0-9]*\//);
    var mailtoRegex = new RegExp(/^mailto:/);
      var filterRegex = new RegExp('/' + window.location.host + '/');
    var isInternal = (href) => {
        return filterRegex.test(href) || localhostRegex.test(href) || mailtoRegex.test(href);
    }
    // Inspect non-navigation links and adorn them if external
 	var links = window.document.querySelectorAll('a[href]:not(.nav-link):not(.navbar-brand):not(.toc-action):not(.sidebar-link):not(.sidebar-item-toggle):not(.pagination-link):not(.no-external):not([aria-hidden]):not(.dropdown-item):not(.quarto-navigation-tool):not(.about-link)');
    for (var i=0; i<links.length; i++) {
      const link = links[i];
      if (!isInternal(link.href)) {
        // undo the damage that might have been done by quarto-nav.js in the case of
        // links that we want to consider external
        if (link.dataset.originalHref !== undefined) {
          link.href = link.dataset.originalHref;
        }
      }
    }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      if (note) {
        return note.innerHTML;
      } else {
        return "";
      }
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->




</body></html>